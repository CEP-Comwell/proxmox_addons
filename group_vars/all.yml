# Default bridge_ports mapping (can be overridden in host_vars)
# bridges:
#   - name: vmbr0
#     bridge_ports: ens1f0
#   - name: vmbr1
#     bridge_ports: ens1f1
#   - name: vmbr2
#     bridge_ports: eno2

# nftables configuration for SDN bridges
nftables_configure_bridges: false
nftables_nat_enabled: true
nftables_nat_external_iface: vmbr0
nftables_nat_source_networks: "{{ sdn_vni_plan | map(attribute='cidr') | list }}"
nftables_explicit_rules:
  - 'iifname { "vmbr1", "vmbr2", "vmbr99" } oifname "vmbr0" accept'
  - 'iifname "vmbr0" oifname { "vmbr1", "vmbr2", "vmbr99" } ct state established,related accept'
  - 'iifname "vmbr1" oifname { "vmbr2", "vmbr99" } accept'
  - 'iifname "vmbr2" oifname { "vmbr1", "vmbr99" } accept'
  - 'iifname "vmbr99" oifname { "vmbr1", "vmbr2" } accept'
  - 'iifname { "vmbr1", "vmbr2", "vmbr99" } oifname { "vmbr1", "vmbr2", "vmbr99" } ct state established,related accept'
# Global variables for all roles and hosts

# Underlay physical interface (used for routed underlay)
underlay_phys_iface: eno1

# BGP configuration
bgp_neighbors:
  - neighbor_ip: 192.168.1.1
    remote_as: 65001
  - neighbor_ip: 192.168.1.2
    remote_as: 65002
local_as: 65000

# Proxy VM IP address
proxy_vm_ip: 172.16.20.47

# NAT source IPs
nat_source_ips:
  - 172.16.20.47

# Ceph/ZFS replication (optional)
ceph_replication_network: 10.10.30.0/24
ceph_cluster_network: 10.10.31.0/24

# --- SDN/EVPN fabric variables for sdn_fabric_provision ---
sdn_controller_name: controller1
sdn_asn: 65000
sdn_peers: "10.0.0.1,10.0.0.2"
sdn_zone_name: zone1
# Existing fabric name as defined in /etc/pve/sdn/sdn.cfg. Update to match manual fabric (for example 'fabric1').
sdn_fabric_name: fab-core
sdn_fabric:
  type: evpn
  controller: controller1
  nodes: []
  extra: {}
sdn_fabric_api:
  protocol: openfabric
  ip_prefix: 10.255.0.0/16
  ip6_prefix: fd00::/48
sdn_fabric_nodes:
  - node_id: pve2
    protocol: openfabric
    ip: 10.255.0.2
    ip6: fd00::2
sdn_l3vni: 100
sdn_zone_mtu: 1400
sdn_anycast_mac: "BC:24:11:D0:C7:8E"
sdn_zone_nodes: []
sdn_vni_plan:
  - name: vx10100
    vni: 10100
    cidr: 10.101.0.0/24
    gw: 10.101.0.1
    bridge: vmbr99
  - name: vx10101
    vni: 10101
    cidr: 10.101.1.0/24
    gw: 10.101.1.1
    bridge: vmbr99
  - name: vx10102
    vni: 10102
    cidr: 10.101.2.0/24
    gw: 10.101.2.1
    bridge: vmbr99
  - name: vx10031
    vni: 10031
    cidr: 10.10.31.0/24
    gw: 10.10.31.1
    bridge: vmbr99
  - name: vx10032
    vni: 10032
    cidr: 10.10.32.0/24
    gw: 10.10.32.1
    bridge: vmbr99
  - name: vx10110
    vni: 10110
    cidr: 10.110.10.0/24
    gw: 10.110.10.1
    bridge: vmbr1
  - name: vx9000
    vni: 9000
    cidr: 10.90.0.0/24
    gw: 10.90.0.1
    bridge: vmbr1
  - name: vx9006
    vni: 9006
    cidr: 10.90.6.0/24
    gw: 10.90.6.1
    bridge: vmbr1
  - name: vx9003
    vni: 9003
    cidr: 10.90.3.0/24
    gw: 10.90.3.1
    bridge: vmbr2
  - name: vx10120
    vni: 10120
    cidr: 10.120.10.0/24
    gw: 10.120.10.1
    bridge: vmbr2

# Proxmox VE 9.0.11 lacks a create/get handler for /cluster/sdn/subnets.
# Set this to true once a future release exposes the API so automation can manage subnets.
sdn_subnet_api_supported: false

# Manage /etc/pve/sdn/sdn.cfg via template when true.
sdn_manage_config_file: true

# Optional raw config block inserted verbatim inside sdn: {} after managed objects.
sdn_extra_config: |
  zone zone2 {
    type evpn
    controller controller1
    fabric fab-core
      vrf-vxlan 200
      mac BC:24:11:F6:1E:41
      mtu 1400
      advertise-subnets 1
  }

  vnet vnet1 {
    zone zone1
    type vnet
    tag 10
  }

  vnet vnet2 {
    zone zone2
    type vnet
    tag 20
  }